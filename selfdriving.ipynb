{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2379,
     "status": "ok",
     "timestamp": 1671893997556,
     "user": {
      "displayName": "Lakshmikar Polamreddy",
      "userId": "02016368727865707919"
     },
     "user_tz": 300
    },
    "id": "Ha-bZF_9xOX4",
    "outputId": "a6286f1c-c07e-4494-997d-8da8d0e47920"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.1+cu102\n",
      "1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 1326,
     "status": "ok",
     "timestamp": 1671894001293,
     "user": {
      "displayName": "Lakshmikar Polamreddy",
      "userId": "02016368727865707919"
     },
     "user_tz": 300
    },
    "id": "QEJoMkGh0e8z"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lakshmikar Reddy\\anaconda3\\envs\\gpu\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: [WinError 126] The specified module could not be found\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\MS_AI_YU\\\\Fall 2022\\\\CV\\\\Project for Conference\\\\My CNN model'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 483,
     "status": "ok",
     "timestamp": 1671903716566,
     "user": {
      "displayName": "Lakshmikar Polamreddy",
      "userId": "02016368727865707919"
     },
     "user_tz": 300
    },
    "id": "qsmxjCvq01At"
   },
   "outputs": [],
   "source": [
    "dataroot = \"./data/dataset/\"\n",
    "valroot = \"./valid/\"\n",
    "ckptroot = \"./\"\n",
    "\n",
    "lr = 1e-4\n",
    "weight_decay = 1e-5\n",
    "batch_size = 32\n",
    "num_workers = 0\n",
    "#test_size = 0.8\n",
    "shuffle = True\n",
    "\n",
    "epochs = 10\n",
    "start_epoch = 0\n",
    "resume = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for test data set\n",
    "testroot = \"./test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1671903720516,
     "user": {
      "displayName": "Lakshmikar Polamreddy",
      "userId": "02016368727865707919"
     },
     "user_tz": 300
    },
    "id": "naCmEozA1BYR"
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def toDevice(datas, device):\n",
    "    \"\"\"Enable cuda.\"\"\"\n",
    "    imgs, angles = datas\n",
    "    return imgs.float().to(device), angles.float().to(device)\n",
    "\n",
    "\n",
    "# def augment(dataroot, imgName, angle):\n",
    "#     \"\"\"Data augmentation.\"\"\"\n",
    "#     name = dataroot + 'IMG/' + imgName.split('\\\\')[-1]\n",
    "#     current_image = cv2.imread(name)\n",
    "#     #current_image = cv2.cvtColor(current_image,cv2.COLOR_RGB2YUV)\n",
    "#     #current_image = cv2.resize(current_image, (128,128))\n",
    "\n",
    "#     if current_image is None:\n",
    "#         print(name)\n",
    "\n",
    "#     current_image = current_image[65:-25, :, :]\n",
    "#     if np.random.rand() < 0.5:\n",
    "#         current_image = cv2.flip(current_image, 1)\n",
    "#         angle = angle * -1.0\n",
    "#     #print(current_image, angle)\n",
    "#     return current_image, angle\n",
    "\n",
    "def augment(dataroot, imgName, angle):\n",
    "    \"\"\"Data augmentation.\"\"\"\n",
    "    name = dataroot + 'IMG/' + imgName.split('\\\\')[-1]\n",
    "    current_image = cv2.imread(name)\n",
    "    current_image = current_image[65:-25, :, :]\n",
    "    \n",
    "    current_image = cv2.cvtColor(current_image,cv2.COLOR_RGB2YUV)\n",
    "\n",
    "    if np.random.rand() < 0.5:\n",
    "        current_image = cv2.flip(current_image, 1)\n",
    "        angle = angle * -1.0\n",
    "    #print(current_image, angle)\n",
    "    return current_image, angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for reading test data set\n",
    "\n",
    "def readtestdata(testroot, imgName, angle):\n",
    "    \"\"\"Reading testdata.\"\"\"\n",
    "    name = testroot + 'IMG/' + imgName.split('\\\\')[-1]\n",
    "    current_image = cv2.imread(name)\n",
    "    \n",
    "    if current_image is None:\n",
    "        print(name)\n",
    "\n",
    "    current_image = current_image[65:-25, :, :]\n",
    "#     if np.random.rand() < 0.5:\n",
    "#         current_image = cv2.flip(current_image, 1)\n",
    "#         angle = angle * -1.0\n",
    "    #print(current_image, angle)\n",
    "    return current_image, angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1671903720516,
     "user": {
      "displayName": "Lakshmikar Polamreddy",
      "userId": "02016368727865707919"
     },
     "user_tz": 300
    },
    "id": "Q03QunDZ1RCQ"
   },
   "outputs": [],
   "source": [
    "# # Data Loading\n",
    "# def load_data(data_dir, test_size):\n",
    "#     \"\"\"Load training data and train validation split\"\"\"\n",
    "#     pass\n",
    "\n",
    "#     # reads CSV file into a single dataframe variable\n",
    "#     data_df = pd.read_csv(os.path.join(data_dir, 'driving_log.csv'),\n",
    "#                           names=['center', 'left', 'right', 'steering', 'throttle', 'reverse', 'speed'])\n",
    "\n",
    "#     # Divide the data into training set and validation set\n",
    "#     train_len = int(test_size * data_df.shape[0])\n",
    "#     valid_len = data_df.shape[0] - train_len\n",
    "#     trainset, valset = data.random_split(\n",
    "#         data_df.values.tolist(), lengths=[train_len, valid_len])\n",
    "\n",
    "#     return trainset, valset\n",
    "\n",
    "# trainset, valset = load_data(dataroot, test_size)\n",
    "# type(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading for training and validation sets\n",
    "trainset = pd.read_csv(os.path.join(dataroot, 'driving_log.csv'),\n",
    "                          names=['center', 'left', 'right', 'steering', 'throttle', 'reverse', 'speed']).values.tolist()\n",
    "valset = pd.read_csv(os.path.join(valroot, 'driving_log.csv'),\n",
    "                          names=['center', 'left', 'right', 'steering', 'throttle', 'reverse', 'speed']).values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Data Loading\n",
    "testset = pd.read_csv(os.path.join(testroot, 'driving_log.csv'),\n",
    "                          names=['center', 'left', 'right', 'steering', 'throttle', 'reverse', 'speed']).values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1671903720516,
     "user": {
      "displayName": "Lakshmikar Polamreddy",
      "userId": "02016368727865707919"
     },
     "user_tz": 300
    },
    "id": "sZVvDuBX13p8"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Creating dataset\n",
    "\n",
    "class TripletDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, dataroot, samples, transform=None):\n",
    "        self.samples = samples\n",
    "        self.dataroot = dataroot\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_samples = self.samples[index]\n",
    "        steering_angle = float(batch_samples[3])\n",
    "\n",
    "        center_img, steering_angle_center = augment(self.dataroot, batch_samples[0], steering_angle)\n",
    "        left_img, steering_angle_left     = augment(self.dataroot, batch_samples[1], steering_angle + 0.4)\n",
    "        right_img, steering_angle_right   = augment(self.dataroot, batch_samples[2], steering_angle - 0.4)\n",
    "\n",
    "        center_img = self.transform(center_img)\n",
    "        left_img   = self.transform(left_img)\n",
    "        right_img  = self.transform(right_img)\n",
    "        \n",
    "        center_img = center_img.transpose((2, 0, 1))\n",
    "        left_img = left_img.transpose((2, 0, 1))\n",
    "        right_img = right_img.transpose((2, 0, 1))\n",
    "\n",
    "        #print(center_img.shape)\n",
    "\n",
    "        return (center_img, steering_angle_center), (left_img, steering_angle_left), (right_img, steering_angle_right)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating test datset\n",
    "\n",
    "class TestDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, testroot, samples, transform=None):\n",
    "        self.samples = samples\n",
    "        self.testroot = testroot\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_samples = self.samples[index]\n",
    "        steering_angle = float(batch_samples[3])\n",
    "\n",
    "        center_img, steering_angle_center = readtestdata(self.testroot, batch_samples[0], steering_angle)\n",
    "        left_img, steering_angle_left     = readtestdata(self.testroot, batch_samples[1], steering_angle + 0.4)\n",
    "        right_img, steering_angle_right   = readtestdata(self.testroot, batch_samples[2], steering_angle - 0.4)\n",
    "\n",
    "        center_img = self.transform(center_img)\n",
    "        left_img   = self.transform(left_img)\n",
    "        right_img  = self.transform(right_img)\n",
    "        \n",
    "        center_img = center_img.transpose((2, 0, 1))\n",
    "        left_img = left_img.transpose((2, 0, 1))\n",
    "        right_img = right_img.transpose((2, 0, 1))\n",
    "\n",
    "        #print(center_img.shape)\n",
    "\n",
    "        return (center_img, steering_angle_center), (left_img, steering_angle_left), (right_img, steering_angle_right)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1671903720516,
     "user": {
      "displayName": "Lakshmikar Polamreddy",
      "userId": "02016368727865707919"
     },
     "user_tz": 300
    },
    "id": "aULGpSEc2Ir_",
    "outputId": "bc38a9e9-4379-4dbf-a0bf-93d9ce2d55c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing dataset ...\n"
     ]
    }
   ],
   "source": [
    "# Data Loader\n",
    "\n",
    "print(\"==> Preparing dataset ...\")\n",
    "def data_loader(dataroot, valroot, trainset, valset, batch_size, shuffle, num_workers):\n",
    "    \"\"\"Self-Driving vehicles simulator dataset Loader.\n",
    "\n",
    "    Args:\n",
    "        trainset: training set\n",
    "        valset: validation set\n",
    "        batch_size: training set input batch size\n",
    "        shuffle: whether shuffle during training process\n",
    "        num_workers: number of workers in DataLoader\n",
    "\n",
    "    Returns:\n",
    "        trainloader (torch.utils.data.DataLoader): DataLoader for training set\n",
    "        testloader (torch.utils.data.DataLoader): DataLoader for validation set\n",
    "    \"\"\"\n",
    "    transformations = transforms.Compose(\n",
    "        [transforms.Lambda(lambda x: (x / 127.5) - 1.0)])\n",
    "\n",
    "    # Load training data and validation data\n",
    "    training_set = TripletDataset(dataroot, trainset, transformations)\n",
    "    trainloader = DataLoader(training_set,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=shuffle,\n",
    "                             num_workers=num_workers)\n",
    "\n",
    "    validation_set = TripletDataset(valroot, valset, transformations)\n",
    "    valloader = DataLoader(validation_set,\n",
    "                           batch_size=batch_size,\n",
    "                           shuffle=shuffle,\n",
    "                           num_workers=num_workers)\n",
    "\n",
    "    return trainloader, valloader\n",
    "\n",
    "\n",
    "trainloader, validationloader = data_loader(dataroot, valroot,\n",
    "                                            trainset, valset,\n",
    "                                            batch_size,\n",
    "                                            shuffle,\n",
    "                                            num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader for test dataset\n",
    "batch_size = 40\n",
    "def testdata_loader(testroot, testset, batch_size, num_workers):\n",
    "    \"\"\"Self-Driving vehicles simulator dataset Loader.\n",
    "\n",
    "    Args:\n",
    "        testset: training set\n",
    "        batch_size: test set input batch size\n",
    "        num_workers: number of workers in DataLoader\n",
    "\n",
    "    Returns:\n",
    "        testloader (torch.utils.data.DataLoader): DataLoader for test set\n",
    "     \"\"\"\n",
    "    transformations = transforms.Compose(\n",
    "        [transforms.Lambda(lambda x: (x / 127.5) - 1.0)])\n",
    "\n",
    "    # Load test data\n",
    "    test_set = TestDataset(testroot, testset, transformations)\n",
    "    #print(test_set.shape())\n",
    "    testloader = DataLoader(test_set,\n",
    "                             batch_size=batch_size,\n",
    "                             num_workers=num_workers)\n",
    "    return testloader\n",
    "\n",
    "\n",
    "testloader = testdata_loader(testroot,\n",
    "                             testset,\n",
    "                             batch_size,\n",
    "                             num_workers)\n",
    "#print(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1671903721412,
     "user": {
      "displayName": "Lakshmikar Polamreddy",
      "userId": "02016368727865707919"
     },
     "user_tz": 300
    },
    "id": "EXBcueyWlrrr",
    "outputId": "72ae1a9b-26bf-471e-92e8-6d1b146a46fd"
   },
   "outputs": [],
   "source": [
    "#Creating model\n",
    "# CNN1 - 3x3 filters, 7 convolution layers\n",
    "\n",
    "# class ConvNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(ConvNet, self).__init__()\n",
    "#         # Input image size = 70x320x3\n",
    "\n",
    "#         self.conv1 = nn.Conv2d(3, 16, 3)  # 68x318\n",
    "#         self.conv2 = nn.Conv2d(16, 32, 3) # 66x316\n",
    "#         self.pool1 = nn.MaxPool2d(2, 2) # 33x158\n",
    "\n",
    "#         self.conv3 = nn.Conv2d(32, 64, 3) # 31x156\n",
    "#         self.conv4 = nn.Conv2d(64, 64, 3) # 29x154\n",
    "#         self.pool2 = nn.MaxPool2d(2, 2) # 14x77\n",
    "\n",
    "#         self.conv5 = nn.Conv2d(64, 64, 3) # 12x75\n",
    "#         self.conv6 = nn.Conv2d(64, 64, 3) # 10x73\n",
    "#         self.conv7 = nn.Conv2d(64, 64, 3) # 8x71\n",
    "#         self.pool3 = nn.MaxPool2d(2, 2) # 4x35\n",
    "\n",
    "        \n",
    "#         self.fc1 = nn.Linear(4*35*64, 128)\n",
    "#         self.dropout1 = nn.Dropout2d(0.25)\n",
    "#         self.fc2 = nn.Linear(128, 64)\n",
    "#         self.dropout2 = nn.Dropout2d(0.5)\n",
    "#         self.fc3 = nn.Linear(64, 1)\n",
    "        \n",
    "\n",
    "#     def forward(self, x):\n",
    "  \n",
    "#         x = F.relu(self.conv1(x))\n",
    "#         x = self.pool1(F.relu(self.conv2(x)))\n",
    "#         x = F.relu(self.conv3(x))\n",
    "#         x = self.pool2(F.relu(self.conv4(x)))\n",
    "#         x = F.elu(self.conv5(x))\n",
    "#         x = F.elu(self.conv6(x))\n",
    "#         x = self.pool3(F.relu(self.conv7(x)))\n",
    "\n",
    "#         x = x.view(-1, 4*35*64)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         #print(x.shape)\n",
    "#         x = self.dropout1(x)\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.dropout2(x)\n",
    "#         x = self.fc3(x)\n",
    "#         return x\n",
    "\n",
    "# # Define model\n",
    "# print(\"==> Initialize model ...\")\n",
    "# model = ConvNet()\n",
    "# print(\"==> Initialize model done ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating model\n",
    "#CNN2 - 6 convolution layers, 3x3 filters, padding-same\n",
    "\n",
    "# class ConvNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(ConvNet, self).__init__()\n",
    "#         # Input image size = 70x320x3\n",
    "\n",
    "#         self.conv1 = nn.Conv2d(3, 16, 3, padding = 'same')  # 70x320\n",
    "#         self.conv2 = nn.Conv2d(16, 32, 3, padding = 'same') # 70x320\n",
    "#         self.pool1 = nn.MaxPool2d(2, 2) # 35x160\n",
    "\n",
    "#         self.conv3 = nn.Conv2d(32, 64, 3, padding = 'same') # 35x160\n",
    "#         self.conv4 = nn.Conv2d(64, 64, 3, padding = 'same') # 35x160\n",
    "#         self.pool2 = nn.MaxPool2d(2, 2) # 17x80\n",
    "\n",
    "#         self.conv5 = nn.Conv2d(64, 64, 3, padding = 'same') # 17x80\n",
    "#         self.conv6 = nn.Conv2d(64, 64, 3, padding = 'same') # 17x80\n",
    "#         self.pool3 = nn.MaxPool2d(2, 2) # 8x40\n",
    "\n",
    "        \n",
    "#         self.fc1 = nn.Linear(8*40*64, 128)\n",
    "#         self.dropout1 = nn.Dropout2d(0.25)\n",
    "#         self.fc2 = nn.Linear(128, 64)\n",
    "#         self.dropout2 = nn.Dropout2d(0.5)\n",
    "#         self.fc3 = nn.Linear(64, 1)\n",
    "        \n",
    "\n",
    "#     def forward(self, x):\n",
    "  \n",
    "#         x = F.relu(self.conv1(x))\n",
    "#         x = self.pool1(F.relu(self.conv2(x)))\n",
    "#         x = F.relu(self.conv3(x))\n",
    "#         x = self.pool2(F.relu(self.conv4(x)))\n",
    "#         x = F.relu(self.conv5(x))\n",
    "#         x = self.pool3(F.relu(self.conv6(x)))\n",
    "\n",
    "#         x = x.view(-1, 8*40*64)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         #print(x.shape)\n",
    "#         x = self.dropout1(x)\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.dropout2(x)\n",
    "#         x = self.fc3(x)\n",
    "#         return x\n",
    "\n",
    "# # Define model\n",
    "# print(\"==> Initialize model ...\")\n",
    "# model = ConvNet()\n",
    "# print(\"==> Initialize model done ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating model\n",
    "# CNN3 - 3 convolution layers, 3x3 filters, padding-same\n",
    "\n",
    "# class ConvNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(ConvNet, self).__init__()\n",
    "#         # Input image size = 70x320x3\n",
    "\n",
    "#         self.conv1 = nn.Conv2d(3, 32, 3, padding = 'same')  # 70x320\n",
    "#         self.pool1 = nn.MaxPool2d(2, 2) # 35x160\n",
    "\n",
    "#         self.conv2 = nn.Conv2d(32, 64, 3, padding = 'same') # 35x160\n",
    "#         self.pool2 = nn.MaxPool2d(2, 2) # 17x80\n",
    "\n",
    "#         self.conv3 = nn.Conv2d(64, 128, 3, padding = 'same') # 17x80\n",
    "#         self.pool3 = nn.MaxPool2d(2, 2) # 8x40\n",
    "#         self.dropout1 = nn.Dropout2d(0.25)\n",
    "\n",
    "        \n",
    "#         self.fc1 = nn.Linear(8*40*128, 512)\n",
    "#         self.dropout2 = nn.Dropout2d(0.25)\n",
    "#         self.fc2 = nn.Linear(512, 1)\n",
    "        \n",
    "\n",
    "#     def forward(self, x):\n",
    "  \n",
    "#         x = self.pool1(F.relu(self.conv1(x)))\n",
    "#         x = self.pool2(F.relu(self.conv2(x)))\n",
    "#         x = self.pool3(F.relu(self.conv3(x)))\n",
    "#         x = self.dropout1(x)\n",
    "\n",
    "#         x = x.view(-1, 8*40*128)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.dropout2(x)\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "# # Define model\n",
    "# print(\"==> Initialize model ...\")\n",
    "# model = ConvNet()\n",
    "# print(\"==> Initialize model done ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating model\n",
    "# CNN4 - 5 convolution layers, 5x5 filters\n",
    "\n",
    "# class ConvNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(ConvNet, self).__init__()\n",
    "#         # Input image size = 70x320x3\n",
    "\n",
    "#         self.conv1 = nn.Conv2d(3, 32, 5, padding = 'same')  # 70x320\n",
    "#         self.pool1 = nn.MaxPool2d(2, 2) # 35x160\n",
    "\n",
    "#         self.conv2 = nn.Conv2d(32, 64, 5, padding = 'same') # 35x160\n",
    "#         self.pool2 = nn.MaxPool2d(2, 2) # 17x80\n",
    "\n",
    "#         self.conv3 = nn.Conv2d(64, 128, 5, padding = 'same') # 17x80\n",
    "#         self.pool3 = nn.MaxPool2d(2, 2) # 8x40\n",
    "        \n",
    "#         self.conv4 = nn.Conv2d(128, 128, 5, padding = 'same') # 8x40\n",
    "#         self.pool4 = nn.MaxPool2d(2, 2) # 4x20\n",
    "        \n",
    "#         self.conv5 = nn.Conv2d(128, 128, 5, padding = 'same') # 4x20\n",
    "#         self.pool5 = nn.MaxPool2d(2, 2) # 2x10\n",
    "        \n",
    "#         self.dropout1 = nn.Dropout2d(0.25)\n",
    "\n",
    "        \n",
    "#         self.fc1 = nn.Linear(2*10*128, 512)\n",
    "#         self.dropout2 = nn.Dropout2d(0.25)\n",
    "#         self.fc2 = nn.Linear(512, 1)\n",
    "        \n",
    "\n",
    "#     def forward(self, x):\n",
    "  \n",
    "#         x = self.pool1(F.relu(self.conv1(x)))\n",
    "#         x = self.pool2(F.relu(self.conv2(x)))\n",
    "#         x = self.pool3(F.relu(self.conv3(x)))\n",
    "#         x = self.pool4(F.relu(self.conv4(x)))\n",
    "#         x = self.pool5(F.relu(self.conv5(x)))\n",
    "#         x = self.dropout1(x)\n",
    "\n",
    "#         x = x.view(-1, 2*10*128)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.dropout2(x)\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "# # Define model\n",
    "# print(\"==> Initialize model ...\")\n",
    "# model = ConvNet()\n",
    "# print(\"==> Initialize model done ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating model\n",
    "# # CNN5 - 5 convolution layers, 7x7 and 5x5 filters\n",
    "\n",
    "# class ConvNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(ConvNet, self).__init__()\n",
    "#         # Input image size = 70x320x3\n",
    "\n",
    "#         self.conv1 = nn.Conv2d(3, 32, 7, padding = 'same')  # 70x320\n",
    "#         self.pool1 = nn.MaxPool2d(2, 2) # 35x160\n",
    "\n",
    "#         self.conv2 = nn.Conv2d(32, 64, 7, padding = 'same') # 35x160\n",
    "#         self.pool2 = nn.MaxPool2d(2, 2) # 17x80\n",
    "\n",
    "#         self.conv3 = nn.Conv2d(64, 128, 7, padding = 'same') # 17x80\n",
    "#         self.pool3 = nn.MaxPool2d(2, 2) # 8x40\n",
    "        \n",
    "#         self.conv4 = nn.Conv2d(128, 128, 5, padding = 'same') # 8x40\n",
    "#         self.pool4 = nn.MaxPool2d(2, 2) # 4x20\n",
    "        \n",
    "#         self.conv5 = nn.Conv2d(128, 128, 5, padding = 'same') # 4x20\n",
    "#         self.pool5 = nn.MaxPool2d(2, 2) # 2x10\n",
    "        \n",
    "#         self.dropout1 = nn.Dropout2d(0.25)\n",
    "\n",
    "        \n",
    "#         self.fc1 = nn.Linear(2*10*128, 512)\n",
    "#         self.dropout2 = nn.Dropout2d(0.25)\n",
    "#         self.fc2 = nn.Linear(512, 1)\n",
    "        \n",
    "\n",
    "#     def forward(self, x):\n",
    "  \n",
    "#         x = self.pool1(F.relu(self.conv1(x)))\n",
    "#         x = self.pool2(F.relu(self.conv2(x)))\n",
    "#         x = self.pool3(F.relu(self.conv3(x)))\n",
    "#         x = self.pool4(F.relu(self.conv4(x)))\n",
    "#         x = self.pool5(F.relu(self.conv5(x)))\n",
    "#         x = self.dropout1(x)\n",
    "\n",
    "#         x = x.view(-1, 2*10*128)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.dropout2(x)\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "# # Define model\n",
    "# print(\"==> Initialize model ...\")\n",
    "# model = ConvNet()\n",
    "# print(\"==> Initialize model done ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating model\n",
    "# # CNN6 - 3 convolution layers, 7x7 filters\n",
    "\n",
    "# class ConvNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(ConvNet, self).__init__()\n",
    "#         # Input image size = 70x320x3\n",
    "\n",
    "#         self.conv1 = nn.Conv2d(3, 16, 7, 2)  # 32x157\n",
    "#         self.conv2 = nn.Conv2d(16, 32, 7, 2) # 13x76\n",
    "#         self.conv3 = nn.Conv2d(32, 64, 7, 2) # 4x35\n",
    "#         self.pool1 = nn.MaxPool2d(2, 2) # 2x17\n",
    "#         self.dropout1 = nn.Dropout2d(0.25)\n",
    "        \n",
    "#         self.fc1 = nn.Linear(2*17*64, 512)\n",
    "#         self.dropout2 = nn.Dropout2d(0.25)\n",
    "#         self.fc2 = nn.Linear(512, 1)\n",
    "        \n",
    "\n",
    "#     def forward(self, x):\n",
    "  \n",
    "#         x = F.relu(self.conv1(x))\n",
    "#         x = F.relu(self.conv2(x))\n",
    "#         x = self.pool1(F.relu(self.conv3(x)))\n",
    "#         x = self.dropout1(x)\n",
    "\n",
    "#         x = x.view(-1, 2*17*64)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.dropout2(x)\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "# # Define model\n",
    "# print(\"==> Initialize model ...\")\n",
    "# model = ConvNet()\n",
    "# # print(\"==> Initialize model done ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating model\n",
    "# CNN7 - 3 convolution layers, 3x3 filters, YUV image\n",
    "\n",
    "\n",
    "#class ConvNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(ConvNet, self).__init__()\n",
    "#         # Input image size = 128x128x3\n",
    "#         # Input image size = 70x320x3\n",
    "\n",
    "#         self.conv1 = nn.Conv2d(3, 16, 3, padding = 'same')  # 70x320\n",
    "#         self.pool1 = nn.MaxPool2d(2, 2) # 35x160\n",
    "\n",
    "#         self.conv2 = nn.Conv2d(16, 32, 3, padding = 'same') # 35x160\n",
    "#         self.pool2 = nn.MaxPool2d(2, 2) # 17x80\n",
    "\n",
    "#         self.conv3 = nn.Conv2d(32, 64, 3, padding = 'same') # 17x80\n",
    "#         self.pool3 = nn.MaxPool2d(2, 2) # 8x40\n",
    "#         self.dropout1 = nn.Dropout2d(0.25)\n",
    "\n",
    "        \n",
    "#         self.fc1 = nn.Linear(8*40*64, 512)\n",
    "#         self.dropout2 = nn.Dropout2d(0.25)\n",
    "#         self.fc2 = nn.Linear(512, 1)\n",
    "        \n",
    "\n",
    "#     def forward(self, x):\n",
    "  \n",
    "#         x = self.pool1(F.relu(self.conv1(x)))\n",
    "#         x = self.pool2(F.relu(self.conv2(x)))\n",
    "#         x = self.pool3(F.relu(self.conv3(x)))\n",
    "#         x = self.dropout1(x)\n",
    "\n",
    "#         x = x.view(-1, 8*40*64)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.dropout2(x)\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "# # Define model\n",
    "# print(\"==> Initialize model ...\")\n",
    "# model = ConvNet()\n",
    "# print(\"==> Initialize model done ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating model\n",
    "# # CNN8 - 4 convolution layers, 3x3 filters, YUV image\n",
    "\n",
    "# class ConvNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(ConvNet, self).__init__()\n",
    "#         # Input image size = 70x320x3\n",
    "\n",
    "#         self.conv1 = nn.Conv2d(3, 16, 3, padding = 'same')  # 70x320\n",
    "#         self.pool1 = nn.MaxPool2d(2, 2) # 35x160\n",
    "\n",
    "#         self.conv2 = nn.Conv2d(16, 32, 3, padding = 'same') # 35x160\n",
    "#         self.pool2 = nn.MaxPool2d(2, 2) # 17x80\n",
    "\n",
    "#         self.conv3 = nn.Conv2d(32, 64, 3, padding = 'same') # 17x80\n",
    "#         self.pool3 = nn.MaxPool2d(2, 2) # 8x40\n",
    "        \n",
    "#         self.conv4 = nn.Conv2d(64, 64, 3, padding = 'same') # 8x40\n",
    "#         self.pool4 = nn.MaxPool2d(2, 2) # 4x20\n",
    "#         self.dropout1 = nn.Dropout2d(0.25)\n",
    "\n",
    "        \n",
    "#         self.fc1 = nn.Linear(4*20*64, 512)\n",
    "#         self.dropout2 = nn.Dropout2d(0.25)\n",
    "#         self.fc2 = nn.Linear(512, 1)\n",
    "        \n",
    "\n",
    "#     def forward(self, x):\n",
    "  \n",
    "#         x = self.pool1(F.relu(self.conv1(x)))\n",
    "#         x = self.pool2(F.relu(self.conv2(x)))\n",
    "#         x = self.pool3(F.relu(self.conv3(x)))\n",
    "#         x = self.pool4(F.relu(self.conv4(x)))\n",
    "#         x = self.dropout1(x)\n",
    "\n",
    "#         x = x.view(-1, 4*20*64)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.dropout2(x)\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "# # Define model\n",
    "# print(\"==> Initialize model ...\")\n",
    "# model = ConvNet()\n",
    "# print(\"==> Initialize model done ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Initialize model ...\n",
      "==> Initialize model done ...\n"
     ]
    }
   ],
   "source": [
    "# Creating model\n",
    "# CNN9 - 4 convolution layers, 3x3 and 5x5 filters, YUV image\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # Input image size = 70x320x3\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding = 'same')  # 70x320\n",
    "        self.pool1 = nn.MaxPool2d(2, 2) # 35x160\n",
    "\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding = 'same') # 35x160\n",
    "        self.pool2 = nn.MaxPool2d(2, 2) # 17x80\n",
    "\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding = 'same') # 17x80\n",
    "        self.pool3 = nn.MaxPool2d(2, 2) # 8x40\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(64, 64, 5, 2) # 2x18, strides=2\n",
    "        self.pool4 = nn.MaxPool2d(2, 2) # 1x9\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "\n",
    "        \n",
    "        self.fc1 = nn.Linear(1*9*64, 256)\n",
    "        self.dropout2 = nn.Dropout2d(0.25)\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "  \n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = self.pool3(F.relu(self.conv3(x)))\n",
    "        x = self.pool4(F.relu(self.conv4(x)))\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = x.view(-1, 1*9*64)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define model\n",
    "print(\"==> Initialize model ...\")\n",
    "model = ConvNet()\n",
    "print(\"==> Initialize model done ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(5, 5), stride=(2, 2))\n",
      "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dropout1): Dropout2d(p=0.25, inplace=False)\n",
      "  (fc1): Linear(in_features=576, out_features=256, bias=True)\n",
      "  (dropout2): Dropout2d(p=0.25, inplace=False)\n",
      "  (fc2): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MAdd]: Dropout2d is not supported!\n",
      "[Flops]: Dropout2d is not supported!\n",
      "[Memory]: Dropout2d is not supported!\n",
      "[MAdd]: Dropout2d is not supported!\n",
      "[Flops]: Dropout2d is not supported!\n",
      "[Memory]: Dropout2d is not supported!\n",
      "      module name  input shape output shape    params memory(MB)           MAdd         Flops  MemRead(B)  MemWrite(B) duration[%]  MemR+W(B)\n",
      "0           conv1    3  70 320   16  70 320     448.0       1.37   19,353,600.0  10,035,200.0    270592.0    1433600.0      48.50%  1704192.0\n",
      "1           pool1   16  70 320   16  35 160       0.0       0.34      268,800.0     358,400.0   1433600.0     358400.0       7.23%  1792000.0\n",
      "2           conv2   16  35 160   32  35 160    4640.0       0.68   51,609,600.0  25,984,000.0    376960.0     716800.0      13.88%  1093760.0\n",
      "3           pool2   32  35 160   32  17  80       0.0       0.17      130,560.0     179,200.0    716800.0     174080.0       4.42%   890880.0\n",
      "4           conv3   32  17  80   64  17  80   18496.0       0.33   50,135,040.0  25,154,560.0    248064.0     348160.0      11.96%   596224.0\n",
      "5           pool3   64  17  80   64   8  40       0.0       0.08       61,440.0      87,040.0    348160.0      81920.0       3.71%   430080.0\n",
      "6           conv4   64   8  40   64   2  18  102464.0       0.01    7,372,800.0   3,688,704.0    491776.0       9216.0       6.85%   500992.0\n",
      "7           pool4   64   2  18   64   1   9       0.0       0.00        1,728.0       2,304.0      9216.0       2304.0       0.00%    11520.0\n",
      "8        dropout1   64   1   9   64   1   9       0.0       0.00            0.0           0.0         0.0          0.0       0.00%        0.0\n",
      "9             fc1          576          256  147712.0       0.00      294,656.0     147,456.0    593152.0       1024.0       3.44%   594176.0\n",
      "10       dropout2          256          256       0.0       0.00            0.0           0.0         0.0          0.0       0.00%        0.0\n",
      "11            fc2          256            1     257.0       0.00          511.0         256.0      2052.0          4.0       0.00%     2056.0\n",
      "total                                        274017.0       2.98  129,228,735.0  65,637,120.0      2052.0          4.0     100.00%  7615880.0\n",
      "=============================================================================================================================================\n",
      "Total params: 274,017\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Total memory: 2.98MB\n",
      "Total MAdd: 129.23MMAdd\n",
      "Total Flops: 65.64MFlops\n",
      "Total MemR+W: 7.26MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculating parameters and memory\n",
    "\n",
    "from torchstat import stat\n",
    "stat(model, (3, 70, 320))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Initialize model ...\n",
      "==> Initialize model done ...\n"
     ]
    }
   ],
   "source": [
    "# # Creating model\n",
    "# # CNN10 - 4 convolution layers, 3x3 and 5x5 filters, 2FCs, YUV image\n",
    "\n",
    "# class ConvNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(ConvNet, self).__init__()\n",
    "#         # Input image size = 70x320x3\n",
    "\n",
    "#         self.conv1 = nn.Conv2d(3, 16, 3, padding = 'same')  # 70x320\n",
    "#         self.pool1 = nn.MaxPool2d(2, 2) # 35x160\n",
    "\n",
    "#         self.conv2 = nn.Conv2d(16, 32, 3, padding = 'same') # 35x160\n",
    "#         self.pool2 = nn.MaxPool2d(2, 2) # 17x80\n",
    "\n",
    "#         self.conv3 = nn.Conv2d(32, 64, 3, padding = 'same') # 17x80\n",
    "#         self.pool3 = nn.MaxPool2d(2, 2) # 8x40\n",
    "        \n",
    "#         self.conv4 = nn.Conv2d(64, 64, 5, 2) # 2x18, strides=2\n",
    "#         self.pool4 = nn.MaxPool2d(2, 2) # 1x9\n",
    "#         self.dropout1 = nn.Dropout2d(0.25)\n",
    "\n",
    "        \n",
    "#         self.fc1 = nn.Linear(1*9*64, 256)\n",
    "#         self.fc2 = nn.Linear(256, 64)\n",
    "#         self.dropout2 = nn.Dropout2d(0.25)\n",
    "#         self.fc3 = nn.Linear(64, 1)\n",
    "        \n",
    "\n",
    "#     def forward(self, x):\n",
    "  \n",
    "#         x = self.pool1(F.relu(self.conv1(x)))\n",
    "#         x = self.pool2(F.relu(self.conv2(x)))\n",
    "#         x = self.pool3(F.relu(self.conv3(x)))\n",
    "#         x = self.pool4(F.relu(self.conv4(x)))\n",
    "#         x = self.dropout1(x)\n",
    "\n",
    "#         x = x.view(-1, 1*9*64)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.dropout2(x)\n",
    "#         x = self.fc3(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "# # Define model\n",
    "# print(\"==> Initialize model ...\")\n",
    "# model = ConvNet()\n",
    "# print(\"==> Initialize model done ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating model\n",
    "# CNN11 - 4 convolution layers, 5x5 and 3x3 filters, 2FCs, YUV image\n",
    "\n",
    "# class ConvNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(ConvNet, self).__init__()\n",
    "#         # Input image size = 70x320x3\n",
    "\n",
    "#         self.conv1 = nn.Conv2d(3, 16, 5, 2)  # 33x158\n",
    "#         self.pool1 = nn.MaxPool2d(2, 2) # 16x79\n",
    "\n",
    "#         self.conv2 = nn.Conv2d(16, 32, 3, padding = 'same') # 16x79\n",
    "#         self.pool2 = nn.MaxPool2d(2, 2) # 8x39\n",
    "\n",
    "#         self.conv3 = nn.Conv2d(32, 64, 3, padding = 'same') # 8x39\n",
    "#         self.pool3 = nn.MaxPool2d(2, 2) # 4x19\n",
    "        \n",
    "#         self.conv4 = nn.Conv2d(64, 64, 3, padding = 'same') # 4x19\n",
    "#         self.pool4 = nn.MaxPool2d(2, 2) # 2x9\n",
    "#         self.dropout1 = nn.Dropout2d(0.25)\n",
    "\n",
    "        \n",
    "#         self.fc1 = nn.Linear(2*9*64, 256)\n",
    "#         self.fc2 = nn.Linear(256, 64)\n",
    "#         self.dropout2 = nn.Dropout2d(0.25)\n",
    "#         self.fc3 = nn.Linear(64, 1)\n",
    "        \n",
    "\n",
    "#     def forward(self, x):\n",
    "  \n",
    "#         x = self.pool1(F.relu(self.conv1(x)))\n",
    "#         x = self.pool2(F.relu(self.conv2(x)))\n",
    "#         x = self.pool3(F.relu(self.conv3(x)))\n",
    "#         x = self.pool4(F.relu(self.conv4(x)))\n",
    "#         x = self.dropout1(x)\n",
    "\n",
    "#         x = x.view(-1, 2*9*64)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.dropout2(x)\n",
    "#         x = self.fc3(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "# # Define model\n",
    "# print(\"==> Initialize model ...\")\n",
    "# model = ConvNet()\n",
    "# print(\"==> Initialize model done ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating model\n",
    "# CNN12 - 4 convolution layers, 5x5 and 3x3 filters, 2FCs, YUV image\n",
    "\n",
    "# class ConvNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(ConvNet, self).__init__()\n",
    "#         # Input image size = 70x320x3\n",
    "\n",
    "#         self.conv1 = nn.Conv2d(3, 16, 5, 2)  # 33x158\n",
    "#         self.conv2 = nn.Conv2d(16, 32, 5, 2) # 15x77\n",
    "#         self.pool1 = nn.MaxPool2d(2, 2) # 7x38\n",
    "\n",
    "#         self.conv3 = nn.Conv2d(32, 64, 3) # 5x36       \n",
    "#         self.conv4 = nn.Conv2d(64, 64, 3) # 3x34\n",
    "#         self.pool2 = nn.MaxPool2d(2, 2) # 1x17\n",
    "#         self.dropout1 = nn.Dropout2d(0.25)\n",
    "\n",
    "        \n",
    "#         self.fc1 = nn.Linear(1*17*64, 256)\n",
    "#         self.fc2 = nn.Linear(256, 64)\n",
    "#         self.dropout2 = nn.Dropout2d(0.25)\n",
    "#         self.fc3 = nn.Linear(64, 1)\n",
    "        \n",
    "\n",
    "#     def forward(self, x):\n",
    "  \n",
    "#         x = F.relu(self.conv1(x))\n",
    "#         x = self.pool1(F.relu(self.conv2(x)))\n",
    "#         x = F.relu(self.conv3(x))\n",
    "#         x = self.pool2(F.relu(self.conv4(x)))\n",
    "#         x = self.dropout1(x)\n",
    "\n",
    "#         x = x.view(-1, 1*17*64)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.dropout2(x)\n",
    "#         x = self.fc3(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "# # Define model\n",
    "# print(\"==> Initialize model ...\")\n",
    "# model = ConvNet()\n",
    "# print(\"==> Initialize model done ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1671903721413,
     "user": {
      "displayName": "Lakshmikar Polamreddy",
      "userId": "02016368727865707919"
     },
     "user_tz": 300
    },
    "id": "z33gtcI52hMJ"
   },
   "outputs": [],
   "source": [
    "# Define optimizer and criterion\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=lr,\n",
    "                       weight_decay=weight_decay)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1671903723297,
     "user": {
      "displayName": "Lakshmikar Polamreddy",
      "userId": "02016368727865707919"
     },
     "user_tz": 300
    },
    "id": "655j9XUm2kud",
    "outputId": "17baaf61-aa09-4de5-e859-0ab06ab26ff5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# learning rate scheduler\n",
    "scheduler = MultiStepLR(optimizer, milestones=[30, 50], gamma=0.1)\n",
    "\n",
    "# transfer to gpu\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1671903723297,
     "user": {
      "displayName": "Lakshmikar Polamreddy",
      "userId": "02016368727865707919"
     },
     "user_tz": 300
    },
    "id": "1VgaFCKk2op7"
   },
   "outputs": [],
   "source": [
    "# Resuming training\n",
    "\n",
    "if resume:\n",
    "    print(\"==> Loading checkpoint ...\")\n",
    "    checkpoint = torch.load(\"../model2_CNN10_YUV-30.h5\",\n",
    "                            map_location=lambda storage, loc: storage)\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1671903725246,
     "user": {
      "displayName": "Lakshmikar Polamreddy",
      "userId": "02016368727865707919"
     },
     "user_tz": 300
    },
    "id": "Ax3HVOa52yFD"
   },
   "outputs": [],
   "source": [
    "# Model training\n",
    "\n",
    "class Trainer(object):\n",
    "    \"\"\"Trainer.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 ckptroot,\n",
    "                 model,\n",
    "                 device,\n",
    "                 epochs,\n",
    "                 criterion,\n",
    "                 optimizer,\n",
    "                 scheduler,\n",
    "                 start_epoch,\n",
    "                 trainloader,\n",
    "                 validationloader):\n",
    "        \"\"\"Self-Driving car Trainer.\n",
    "\n",
    "        Args:\n",
    "            model:\n",
    "            device:\n",
    "            epochs:\n",
    "            criterion:\n",
    "            optimizer:\n",
    "            start_epoch:\n",
    "            trainloader:\n",
    "            validationloader:\n",
    "\n",
    "        \"\"\"\n",
    "        super(Trainer, self).__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.epochs = epochs\n",
    "        self.ckptroot = ckptroot\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.start_epoch = start_epoch\n",
    "        self.trainloader = trainloader\n",
    "        self.validationloader = validationloader\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Training process.\"\"\"\n",
    "        self.model.to(self.device)\n",
    "        Training_loss = []\n",
    "        Validation_loss = []\n",
    "        epochs = []\n",
    "\n",
    "        for epoch in range(self.start_epoch, self.epochs + self.start_epoch):\n",
    "            epochs.append(epoch)\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            # Training\n",
    "            train_loss = 0.0\n",
    "            self.model.train()\n",
    "            #print(\"Before train loading\")\n",
    "\n",
    "            for local_batch, (centers, lefts, rights) in enumerate(self.trainloader):\n",
    "                #print(\"After train loading\")\n",
    "                # Transfer to GPU\n",
    "                centers, lefts, rights = toDevice(centers, self.device), toDevice(\n",
    "                    lefts, self.device), toDevice(rights, self.device)\n",
    "\n",
    "                # Model computations\n",
    "                self.optimizer.zero_grad()\n",
    "                datas = [centers, lefts, rights]\n",
    "                for data in datas:\n",
    "                    imgs, angles = data\n",
    "                    # print(\"training image: \", imgs.shape)\n",
    "                    outputs = self.model(imgs)\n",
    "                    loss = self.criterion(outputs, angles.unsqueeze(1))\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                    train_loss += loss.data.item()\n",
    "\n",
    "                if local_batch % 100 == 0:\n",
    "\n",
    "                    print(\"Training Epoch: {} | Loss: {}\".format(epoch, train_loss / (local_batch + 1)))\n",
    "                    Training_loss.append(train_loss)\n",
    "\n",
    "\n",
    "            # Validation\n",
    "            self.model.eval()\n",
    "            valid_loss = 0\n",
    "            with torch.set_grad_enabled(False):\n",
    "                for local_batch, (centers, lefts, rights) in enumerate(self.validationloader):\n",
    "                    # Transfer to GPU\n",
    "                    centers, lefts, rights = toDevice(centers, self.device), toDevice(\n",
    "                        lefts, self.device), toDevice(rights, self.device)\n",
    "\n",
    "                    # Model computations\n",
    "                    self.optimizer.zero_grad()\n",
    "                    datas = [centers, lefts, rights]\n",
    "                    predicts = []\n",
    "                    actuals = []\n",
    "                    for data in datas:\n",
    "                        imgs, angles = data\n",
    "                        #print('Angles:', angles)\n",
    "                        actuals.append(angles.cpu().squeeze())\n",
    "                        outputs = self.model(imgs)\n",
    "                        #print('Outputs: ', outputs.reshape(-1))\n",
    "                        #x = pd.DataFrame(angles.cpu().squeeze(), columns = ['Actual angles'])\n",
    "                        #y = pd.DataFrame(outputs.reshape(-1).cpu().squeeze(), columns = ['Predicted angles'])\n",
    "                        #print(pd.concat([x,y], axis =1))\n",
    "                        loss = self.criterion(outputs, angles.unsqueeze(1))\n",
    "                        predicts.append(outputs.reshape(-1).cpu().squeeze())\n",
    "\n",
    "                        valid_loss += loss.data.item()\n",
    "\n",
    "                    if local_batch % 100 == 0:\n",
    "                        print(\"Validation Loss: {}\".format(valid_loss / (local_batch + 1)))\n",
    "                        Validation_loss.append(valid_loss)\n",
    "                        \n",
    "\n",
    "            \n",
    "            #plt.plot(Validation_loss,epochs)\n",
    "            #print('actuals shape:', len(actuals))\n",
    "            #print('predicts shape:', len(predicts))\n",
    "\n",
    "            x = pd.DataFrame(actuals[1], columns = ['Actual Angles'])\n",
    "            y = pd.DataFrame(predicts[1], columns = ['Predicted Angles'])\n",
    "            #print(x)\n",
    "            #print(y)\n",
    "            print(pd.concat([x,y], axis=1))\n",
    "            print('MSE:',mean_squared_error(actuals[1],predicts[1]))\n",
    "            print()\n",
    "            # Save model\n",
    "                        \n",
    "#             if epoch % 5 == 0 or epoch == self.epochs + self.start_epoch - 1:\n",
    "\n",
    "#                 state = {\n",
    "#                     'epoch': epoch + 1,\n",
    "#                     'state_dict': self.model.state_dict(),\n",
    "#                     'optimizer': self.optimizer.state_dict(),\n",
    "#                     'scheduler': self.scheduler.state_dict(),\n",
    "#                 }\n",
    "\n",
    "#                 self.save_checkpoint(state)\n",
    "\n",
    "\n",
    "\n",
    "            state = {\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': self.model.state_dict(),\n",
    "                'optimizer': self.optimizer.state_dict(),\n",
    "                'scheduler': self.scheduler.state_dict(),\n",
    "            }\n",
    "\n",
    "            self.save_checkpoint(state)\n",
    "\n",
    "    def save_checkpoint(self, state):\n",
    "        \"\"\"Save checkpoint.\"\"\"\n",
    "        print(\"==> Save checkpoint ...\")\n",
    "        if not os.path.exists(self.ckptroot):\n",
    "            os.makedirs(self.ckptroot)\n",
    "\n",
    "        torch.save(state, self.ckptroot + 'model2_CNN10_YUV-{}.h5'.format(state['epoch']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6274098,
     "status": "ok",
     "timestamp": 1671909999340,
     "user": {
      "displayName": "Lakshmikar Polamreddy",
      "userId": "02016368727865707919"
     },
     "user_tz": 300
    },
    "id": "6Llhe9UX2yHb",
    "outputId": "012a5e15-a698-4e77-d415-05d992f2e36b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Start training ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lakshmikar Reddy\\anaconda3\\envs\\gpu\\lib\\site-packages\\torch\\nn\\functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 0 | Loss: 0.24815470352768898\n",
      "Training Epoch: 0 | Loss: 0.26865641206576685\n",
      "Training Epoch: 0 | Loss: 0.2738899273695934\n",
      "Training Epoch: 0 | Loss: 0.2762955454212189\n",
      "Training Epoch: 0 | Loss: 0.27610485501438753\n",
      "Training Epoch: 0 | Loss: 0.2796681000518347\n",
      "Training Epoch: 0 | Loss: 0.27858361703174783\n",
      "Training Epoch: 0 | Loss: 0.27840109794970586\n",
      "Training Epoch: 0 | Loss: 0.2786810656569982\n",
      "Training Epoch: 0 | Loss: 0.2790290174652284\n",
      "Training Epoch: 0 | Loss: 0.27874585562995025\n",
      "Training Epoch: 0 | Loss: 0.2785858698662956\n",
      "Training Epoch: 0 | Loss: 0.2782615296617088\n",
      "Training Epoch: 0 | Loss: 0.2789996483310954\n",
      "Validation Loss: 0.29314326494932175\n",
      "    Actual Angles  Predicted Angles\n",
      "0            0.00          0.181924\n",
      "1            0.40          0.140762\n",
      "2            0.05         -0.349173\n",
      "3           -0.40         -0.315354\n",
      "4            0.40          0.367883\n",
      "5           -0.40         -0.431894\n",
      "6           -0.40         -0.034802\n",
      "7            0.40          0.160928\n",
      "8            0.40          0.332934\n",
      "9           -0.20          0.081948\n",
      "10           0.40          0.112630\n",
      "11          -0.40          0.014547\n",
      "12          -0.30         -0.317619\n",
      "MSE: 0.061393198\n",
      "\n",
      "==> Save checkpoint ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lakshmikar Reddy\\anaconda3\\envs\\gpu\\lib\\site-packages\\torch\\nn\\functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 | Loss: 0.19705524295568466\n",
      "Training Epoch: 1 | Loss: 0.26546599205103844\n",
      "Training Epoch: 1 | Loss: 0.26420496908289876\n",
      "Training Epoch: 1 | Loss: 0.2670305301481703\n",
      "Training Epoch: 1 | Loss: 0.26932243314430004\n",
      "Training Epoch: 1 | Loss: 0.26953937340074197\n",
      "Training Epoch: 1 | Loss: 0.27098355856964274\n",
      "Training Epoch: 1 | Loss: 0.2723920789843534\n",
      "Training Epoch: 1 | Loss: 0.2720308697941747\n",
      "Training Epoch: 1 | Loss: 0.2724969223985132\n",
      "Training Epoch: 1 | Loss: 0.2730293536929043\n",
      "Training Epoch: 1 | Loss: 0.2737912609614939\n",
      "Training Epoch: 1 | Loss: 0.27510799809694786\n",
      "Training Epoch: 1 | Loss: 0.2757804834818629\n",
      "Validation Loss: 0.18306756392121315\n",
      "    Actual Angles  Predicted Angles\n",
      "0            0.60         -0.036459\n",
      "1            0.40          0.211845\n",
      "2            0.40          0.089062\n",
      "3            0.40          0.393315\n",
      "4           -0.40         -0.305703\n",
      "5            0.40          0.148402\n",
      "6            0.40          0.101375\n",
      "7           -0.75         -0.523649\n",
      "8           -0.40         -0.187499\n",
      "9            0.40          0.070932\n",
      "10          -0.05         -0.349805\n",
      "11          -0.40         -0.266240\n",
      "12          -0.40         -0.097360\n",
      "MSE: 0.08481717\n",
      "\n",
      "==> Save checkpoint ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lakshmikar Reddy\\anaconda3\\envs\\gpu\\lib\\site-packages\\torch\\nn\\functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 | Loss: 0.3698232173919678\n",
      "Training Epoch: 2 | Loss: 0.26690247162512626\n",
      "Training Epoch: 2 | Loss: 0.2703316163327267\n",
      "Training Epoch: 2 | Loss: 0.27152232531198234\n",
      "Training Epoch: 2 | Loss: 0.2746893037621517\n",
      "Training Epoch: 2 | Loss: 0.27266672795671903\n",
      "Training Epoch: 2 | Loss: 0.2718610180055292\n",
      "Training Epoch: 2 | Loss: 0.27075024742829734\n",
      "Training Epoch: 2 | Loss: 0.27079943597512074\n",
      "Training Epoch: 2 | Loss: 0.27216021583666744\n",
      "Training Epoch: 2 | Loss: 0.27204988413414993\n",
      "Training Epoch: 2 | Loss: 0.2718013684171422\n",
      "Training Epoch: 2 | Loss: 0.2719639289882236\n",
      "Training Epoch: 2 | Loss: 0.2721965920014005\n",
      "Validation Loss: 0.29972516000270844\n",
      "    Actual Angles  Predicted Angles\n",
      "0            0.40          0.072370\n",
      "1            0.40          0.271471\n",
      "2            0.40          0.280283\n",
      "3           -0.40         -0.330808\n",
      "4            0.25          0.080296\n",
      "5            0.40          0.139459\n",
      "6            0.40          0.066084\n",
      "7            0.40          0.341133\n",
      "8            0.40         -0.173405\n",
      "9           -0.00         -0.147099\n",
      "10           0.40          0.028069\n",
      "11          -0.40         -0.107309\n",
      "12           0.40          0.330667\n",
      "MSE: 0.07183584\n",
      "\n",
      "==> Save checkpoint ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lakshmikar Reddy\\anaconda3\\envs\\gpu\\lib\\site-packages\\torch\\nn\\functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 | Loss: 0.2771742045879364\n",
      "Training Epoch: 3 | Loss: 0.2731325904940172\n",
      "Training Epoch: 3 | Loss: 0.27434147600158676\n",
      "Training Epoch: 3 | Loss: 0.27256568631720407\n",
      "Training Epoch: 3 | Loss: 0.27161760454732653\n",
      "Training Epoch: 3 | Loss: 0.2684769714697303\n",
      "Training Epoch: 3 | Loss: 0.26964854891760626\n",
      "Training Epoch: 3 | Loss: 0.2700815414850429\n",
      "Training Epoch: 3 | Loss: 0.26902205433659304\n",
      "Training Epoch: 3 | Loss: 0.26984845741768754\n",
      "Training Epoch: 3 | Loss: 0.26867184504684244\n",
      "Training Epoch: 3 | Loss: 0.2686853779980627\n",
      "Training Epoch: 3 | Loss: 0.26890031189784147\n",
      "Training Epoch: 3 | Loss: 0.2689217121292372\n",
      "Validation Loss: 0.22599384561181068\n",
      "    Actual Angles  Predicted Angles\n",
      "0           -0.40         -0.304302\n",
      "1           -0.40         -0.042755\n",
      "2           -0.40         -0.349150\n",
      "3           -0.20         -0.115251\n",
      "4           -0.40         -0.374403\n",
      "5           -0.40         -0.326112\n",
      "6            0.40          0.102931\n",
      "7            0.40          0.242190\n",
      "8           -0.35         -0.143317\n",
      "9           -0.40         -0.073221\n",
      "10           0.15          0.242434\n",
      "11          -0.40         -0.309948\n",
      "12          -0.40         -0.324992\n",
      "MSE: 0.03366165\n",
      "\n",
      "==> Save checkpoint ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lakshmikar Reddy\\anaconda3\\envs\\gpu\\lib\\site-packages\\torch\\nn\\functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4 | Loss: 0.2708005756139755\n",
      "Training Epoch: 4 | Loss: 0.260107786569855\n",
      "Training Epoch: 4 | Loss: 0.2649197504573052\n",
      "Training Epoch: 4 | Loss: 0.26539319715280074\n",
      "Training Epoch: 4 | Loss: 0.26417456038221904\n",
      "Training Epoch: 4 | Loss: 0.26283405909311036\n",
      "Training Epoch: 4 | Loss: 0.2643777608078054\n",
      "Training Epoch: 4 | Loss: 0.2628201950293635\n",
      "Training Epoch: 4 | Loss: 0.26339000951354574\n",
      "Training Epoch: 4 | Loss: 0.2636599257964637\n",
      "Training Epoch: 4 | Loss: 0.26438659604836057\n",
      "Training Epoch: 4 | Loss: 0.2644943250419333\n",
      "Training Epoch: 4 | Loss: 0.2651576335502356\n",
      "Training Epoch: 4 | Loss: 0.2648252975138378\n",
      "Validation Loss: 0.20132038742303848\n",
      "    Actual Angles  Predicted Angles\n",
      "0           -0.40         -0.231972\n",
      "1            0.40          0.348396\n",
      "2           -0.40         -0.257817\n",
      "3            0.40          0.124564\n",
      "4            0.40          0.152184\n",
      "5           -0.40         -0.243645\n",
      "6            0.40          0.341933\n",
      "7            0.40          0.189204\n",
      "8            0.40          0.314221\n",
      "9            0.40          0.052709\n",
      "10          -0.25          0.204714\n",
      "11           0.00          0.098942\n",
      "12          -0.40         -0.254447\n",
      "MSE: 0.048180975\n",
      "\n",
      "==> Save checkpoint ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lakshmikar Reddy\\anaconda3\\envs\\gpu\\lib\\site-packages\\torch\\nn\\functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 5 | Loss: 0.24821829423308372\n",
      "Training Epoch: 5 | Loss: 0.26101783190918443\n",
      "Training Epoch: 5 | Loss: 0.26288845116708115\n",
      "Training Epoch: 5 | Loss: 0.25963448599426453\n",
      "Training Epoch: 5 | Loss: 0.2617491230350034\n",
      "Training Epoch: 5 | Loss: 0.26160425332536597\n",
      "Training Epoch: 5 | Loss: 0.2624005491716691\n",
      "Training Epoch: 5 | Loss: 0.2632648627195353\n",
      "Training Epoch: 5 | Loss: 0.26328017481703886\n",
      "Training Epoch: 5 | Loss: 0.2631992314622782\n",
      "Training Epoch: 5 | Loss: 0.26394022972165765\n",
      "Training Epoch: 5 | Loss: 0.26308401657452807\n",
      "Training Epoch: 5 | Loss: 0.262896341024328\n",
      "Training Epoch: 5 | Loss: 0.2623686795062074\n",
      "Validation Loss: 0.3098125532269478\n",
      "    Actual Angles  Predicted Angles\n",
      "0           -0.40         -0.027248\n",
      "1           -0.40         -0.210051\n",
      "2            0.40          0.222804\n",
      "3           -0.40         -0.208513\n",
      "4           -0.05         -0.093464\n",
      "5           -0.40         -0.363142\n",
      "6           -0.40         -0.220708\n",
      "7            0.95         -0.018677\n",
      "8           -0.30         -0.148159\n",
      "9            0.40          0.300570\n",
      "10          -0.40         -0.191610\n",
      "11          -0.40         -0.135218\n",
      "12          -0.40         -0.258133\n",
      "MSE: 0.106417075\n",
      "\n",
      "==> Save checkpoint ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lakshmikar Reddy\\anaconda3\\envs\\gpu\\lib\\site-packages\\torch\\nn\\functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 6 | Loss: 0.21742912381887436\n",
      "Training Epoch: 6 | Loss: 0.2626621654755113\n",
      "Training Epoch: 6 | Loss: 0.2685207242136867\n",
      "Training Epoch: 6 | Loss: 0.26280719034398514\n",
      "Training Epoch: 6 | Loss: 0.260970201803153\n",
      "Training Epoch: 6 | Loss: 0.2620371342478636\n",
      "Training Epoch: 6 | Loss: 0.2616778206879803\n",
      "Training Epoch: 6 | Loss: 0.26124151298688586\n",
      "Training Epoch: 6 | Loss: 0.2600517875041706\n",
      "Training Epoch: 6 | Loss: 0.2590168007317678\n",
      "Training Epoch: 6 | Loss: 0.259948323182129\n",
      "Training Epoch: 6 | Loss: 0.2611967180817507\n",
      "Training Epoch: 6 | Loss: 0.2607318856532404\n",
      "Training Epoch: 6 | Loss: 0.26137389360335034\n",
      "Validation Loss: 0.22039690241217613\n",
      "    Actual Angles  Predicted Angles\n",
      "0           -0.40         -0.232429\n",
      "1           -0.35         -0.311251\n",
      "2           -0.40         -0.327677\n",
      "3           -0.60          0.136200\n",
      "4            0.20         -0.085189\n",
      "5            0.40          0.374802\n",
      "6            0.40          0.038107\n",
      "7            0.40          0.341846\n",
      "8            0.40          0.027770\n",
      "9            0.40          0.269207\n",
      "10          -0.40         -0.259264\n",
      "11           0.40          0.382229\n",
      "12          -0.30          0.046718\n",
      "MSE: 0.08377827\n",
      "\n",
      "==> Save checkpoint ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lakshmikar Reddy\\anaconda3\\envs\\gpu\\lib\\site-packages\\torch\\nn\\functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 7 | Loss: 0.3338640108704567\n",
      "Training Epoch: 7 | Loss: 0.2522893071285274\n",
      "Training Epoch: 7 | Loss: 0.25246861198018145\n",
      "Training Epoch: 7 | Loss: 0.25451842945477493\n",
      "Training Epoch: 7 | Loss: 0.2550532904330483\n",
      "Training Epoch: 7 | Loss: 0.2559753583443022\n",
      "Training Epoch: 7 | Loss: 0.2575274114303303\n",
      "Training Epoch: 7 | Loss: 0.25752740234306637\n",
      "Training Epoch: 7 | Loss: 0.25674955168513397\n",
      "Training Epoch: 7 | Loss: 0.2569564184283509\n",
      "Training Epoch: 7 | Loss: 0.25758796367984077\n",
      "Training Epoch: 7 | Loss: 0.2584254981731686\n",
      "Training Epoch: 7 | Loss: 0.2582095854734997\n",
      "Training Epoch: 7 | Loss: 0.25772454284223156\n",
      "Validation Loss: 0.2795211225748062\n",
      "    Actual Angles  Predicted Angles\n",
      "0             0.4          0.115153\n",
      "1            -0.4         -0.406224\n",
      "2            -0.2         -0.063061\n",
      "3             0.4          0.066443\n",
      "4             0.4          0.363991\n",
      "5            -0.4         -0.495766\n",
      "6            -0.4         -0.387823\n",
      "7             0.4          0.439620\n",
      "8            -0.4         -0.307205\n",
      "9             0.4          0.158012\n",
      "10            0.4          0.129943\n",
      "11            0.4          0.465436\n",
      "12           -0.4         -0.200580\n",
      "MSE: 0.031348076\n",
      "\n",
      "==> Save checkpoint ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lakshmikar Reddy\\anaconda3\\envs\\gpu\\lib\\site-packages\\torch\\nn\\functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 8 | Loss: 0.24499327689409256\n",
      "Training Epoch: 8 | Loss: 0.2574671365800175\n",
      "Training Epoch: 8 | Loss: 0.25692689686605885\n",
      "Training Epoch: 8 | Loss: 0.2539384611804917\n",
      "Training Epoch: 8 | Loss: 0.2542176183238811\n",
      "Training Epoch: 8 | Loss: 0.25214214726888434\n",
      "Training Epoch: 8 | Loss: 0.25006238381206347\n",
      "Training Epoch: 8 | Loss: 0.250842076096519\n",
      "Training Epoch: 8 | Loss: 0.2517654741968908\n",
      "Training Epoch: 8 | Loss: 0.2520468036047742\n",
      "Training Epoch: 8 | Loss: 0.25288651851503613\n",
      "Training Epoch: 8 | Loss: 0.2533880049163149\n",
      "Training Epoch: 8 | Loss: 0.25327888998954395\n",
      "Training Epoch: 8 | Loss: 0.2536947560159462\n",
      "Validation Loss: 0.24535883218050003\n",
      "    Actual Angles  Predicted Angles\n",
      "0            0.60         -0.225192\n",
      "1           -0.40         -0.276227\n",
      "2            0.40          0.445904\n",
      "3            0.40          0.391239\n",
      "4            0.40          0.210898\n",
      "5            0.40         -0.007980\n",
      "6           -0.40          0.009559\n",
      "7            0.35          0.038927\n",
      "8            0.40          0.125804\n",
      "9           -0.40         -0.288202\n",
      "10          -1.05         -0.457925\n",
      "11          -0.40         -0.095875\n",
      "12           0.40          0.260366\n",
      "MSE: 0.13195258\n",
      "\n",
      "==> Save checkpoint ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lakshmikar Reddy\\anaconda3\\envs\\gpu\\lib\\site-packages\\torch\\nn\\functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 9 | Loss: 0.2293303906917572\n",
      "Training Epoch: 9 | Loss: 0.23459088317312227\n",
      "Training Epoch: 9 | Loss: 0.2389694579427515\n",
      "Training Epoch: 9 | Loss: 0.2391717255152638\n",
      "Training Epoch: 9 | Loss: 0.2401826148875931\n",
      "Training Epoch: 9 | Loss: 0.24119076896421685\n",
      "Training Epoch: 9 | Loss: 0.24095386203483812\n",
      "Training Epoch: 9 | Loss: 0.24016347383487752\n",
      "Training Epoch: 9 | Loss: 0.24017457195909953\n",
      "Training Epoch: 9 | Loss: 0.23971356755199033\n",
      "Training Epoch: 9 | Loss: 0.23911461348996146\n",
      "Training Epoch: 9 | Loss: 0.23876904050729067\n",
      "Training Epoch: 9 | Loss: 0.23782186687448614\n",
      "Training Epoch: 9 | Loss: 0.23715455577043754\n",
      "Validation Loss: 0.21211335808038712\n",
      "    Actual Angles  Predicted Angles\n",
      "0            -0.4         -0.357181\n",
      "1            -0.3         -0.176965\n",
      "2            -0.4         -0.086708\n",
      "3             0.4         -0.001373\n",
      "4            -0.4          0.049965\n",
      "5             0.4         -0.005352\n",
      "6            -0.6          0.307031\n",
      "7            -0.4         -0.198034\n",
      "8             0.3          0.186762\n",
      "9            -0.4         -0.097390\n",
      "10           -0.4         -0.129323\n",
      "11            0.4          0.351524\n",
      "12            0.4          0.031035\n",
      "MSE: 0.14020342\n",
      "\n",
      "==> Save checkpoint ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"==> Start training ...\")\n",
    "trainer = Trainer(ckptroot,\n",
    "                  model,\n",
    "                  device,\n",
    "                  epochs,\n",
    "                  criterion,\n",
    "                  optimizer,\n",
    "                  scheduler,\n",
    "                  start_epoch,\n",
    "                  trainloader,\n",
    "                  validationloader)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {\n",
    "        'model': model.module if device == 'cuda' else model,\n",
    "        }\n",
    "\n",
    "# torch.save(state, 'model' + model_name+ '.h5')\n",
    "torch.save(state, 'model2_CNN9.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "FETsPKWY6xBF",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading weights for Testing\n",
    "\n",
    "state = torch.load(\"./model2_CNN10_YUV-19.h5\")\n",
    "model.load_state_dict(state['state_dict'])\n",
    "#optimizer.load_state_dict(state['optimizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lakshmikar Reddy\\anaconda3\\envs\\gpu\\lib\\site-packages\\torch\\nn\\functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Actual Angles  Predicted Angles\n",
      "0       -0.340585         -0.122153\n",
      "1        0.000000         -0.164918\n",
      "2        0.000000         -0.176276\n",
      "3       -0.071721         -0.137472\n",
      "4       -0.417630         -0.102983\n",
      "5       -0.116112         -0.132684\n",
      "6        0.000000         -0.121567\n",
      "7        0.000000         -0.114900\n",
      "8        0.000000         -0.105712\n",
      "9        0.000000         -0.191875\n",
      "10       0.000000         -0.143677\n",
      "11       0.000000         -0.179033\n",
      "12       0.000000         -0.212254\n",
      "13      -0.071269         -0.207851\n",
      "14      -0.391136         -0.189488\n",
      "15      -0.279243         -0.191007\n",
      "16       0.000000         -0.184803\n",
      "17       0.000000         -0.222327\n",
      "18       0.000000         -0.215403\n",
      "19       0.000000         -0.226560\n",
      "20       0.000000         -0.237701\n",
      "21       0.000000         -0.236624\n",
      "22       0.000000         -0.227489\n",
      "23       0.000000         -0.247275\n",
      "24       0.000000         -0.258622\n",
      "25       0.000000         -0.264047\n",
      "26       0.000000         -0.262567\n",
      "27       0.000000         -0.290204\n",
      "28       0.000000         -0.303155\n",
      "29       0.000000         -0.315684\n",
      "MSE: 0.044614308\n"
     ]
    }
   ],
   "source": [
    "# Testing to calculate MSE\n",
    "\n",
    "with torch.set_grad_enabled(False):\n",
    "    #print(\"Hi\")\n",
    "    for local_batch, (centers, lefts, rights) in enumerate(testloader):\n",
    "        #print(\"Hi all\")\n",
    "        centers, lefts, rights = toDevice(centers, device), toDevice(lefts, device), toDevice(rights, device) # Transfer to GPU\n",
    "\n",
    "        # Model computations\n",
    "        optimizer.zero_grad()\n",
    "        datas = [centers, lefts, rights]\n",
    "        predicts = []\n",
    "        actuals = []\n",
    "        for data in datas:\n",
    "            imgs, angles = data\n",
    "            actuals.append(angles.cpu().squeeze())\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, angles.unsqueeze(1))\n",
    "            predicts.append(outputs.reshape(-1).cpu().squeeze())\n",
    "            \n",
    "x = pd.DataFrame(actuals[0], columns = ['Actual Angles'])\n",
    "y = pd.DataFrame(predicts[0], columns = ['Predicted Angles'])\n",
    "print(pd.concat([x,y], axis=1))\n",
    "print('MSE:',mean_squared_error(actuals[0],predicts[0]))\n",
    "#print(actuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "1RvD4fY5vkdARjOSoZtrd4tewzhbJXApX",
     "timestamp": 1671573948315
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
